{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector search in Python (Azure AI Search)\n",
    "\n",
    "This code demonstrates how to use Azure AI Search by using the push API to insert vectors into your search index:\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-ada-002 model\n",
    "+ Index the vector and nonvector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "The code reads the pdf documents int the data directory, which contains the input files for which embeddings need to be generated.\n",
    "\n",
    "The output is a combination of human-readable text and embeddings that can be pushed into a search index.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ An Azure subscription, with [access to Azure OpenAI](https://aka.ms/oai/access). You must have the Azure OpenAI service name and an API key.\n",
    "\n",
    "+ A deployment of the text-embedding-ada-002 embedding model.\n",
    "\n",
    "+ Azure AI Search, any tier, but choose a service that has sufficient capacity for your vector index. We recommend Basic or higher. [Enable semantic ranking](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable) if you want to run the hybrid query with semantic ranking.\n",
    "\n",
    "We used Python 3.11, [Visual Studio Code with the Python extension](https://code.visualstudio.com/docs/python/python-tutorial), and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) to test this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a Python virtual environment in Visual Studio Code\n",
    "\n",
    "1. Open the Command Palette (Ctrl+Shift+P).\n",
    "1. Search for **Python: Create Environment**.\n",
    "1. Select **Venv**.\n",
    "1. Select a Python interpreter. Choose 3.10 or later.\n",
    "\n",
    "It can take a minute to set up. If you run into problems, see [Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "# The following variables from your .env file are used in this notebook\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"] if len(os.environ[\"AZURE_OPENAI_KEY\"]) > 0 else None\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
    "embedding_model_name = os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\"]\n",
    "azure_openai_api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "blob_container = os.environ.get(\"AZURE_BLOB_CONTAINER\")\n",
    "blob_connection_string = os.environ.get(\"AZURE_BLOB_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import json\n",
    "\n",
    "openai_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(openai_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_deployment=azure_openai_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key,\n",
    "    azure_ad_token_provider=token_provider if not azure_openai_key else None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "\n",
    "Create your search index schema and vector search configuration. If you get an error, check the search service for available quota and check the .env file to make sure you're using a unique search index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sample-docs index created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters\n",
    ")\n",
    "\n",
    "\n",
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=endpoint, credential=credential)\n",
    "fields=[\n",
    "    SearchField(name=\"chunk_id\",type=SearchFieldDataType.String,key=True,filterable=True,sortable=True,searchable=True,analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"parent_id\",type=SearchFieldDataType.String,filterable=True,sortable=True,searchable=True),\n",
    "    SearchField(name=\"chunk\",type=SearchFieldDataType.String,searchable=True),\n",
    "    SearchField(name=\"title\",type=SearchFieldDataType.String,searchable=True),\n",
    "    SearchField(name=\"vector\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True,vector_search_dimensions=1536,vector_search_profile_name=\"profile\"\n",
    "                )\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"hnsw-algorithm\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"profile\",\n",
    "            algorithm_configuration_name=\"hnsw-algorithm\",\n",
    "            # vectorizer=\"azure-openai-vectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    # vectorizers=[\n",
    "    #     AzureOpenAIVectorizer(\n",
    "    #             name=\"azure-openai-vectorizer\",\n",
    "    #             azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "    #                 resource_uri=azure_openai_endpoint,\n",
    "    #                 deployment_id=azure_openai_embedding_deployment,\n",
    "    #                 api_key=azure_openai_key # Optional if using RBAC authentication\n",
    "    #             )\n",
    "    #         )\n",
    "    # ]\n",
    ")\n",
    "\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search,\n",
    "                    semantic_search=semantic_search\n",
    "                    )\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} index created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure AI Search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf\n",
      "Generating embeddings for Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf\n",
      "Generating embeddings for LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "Generating embeddings for Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf\n",
      "Generating embeddings for Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf\n",
      "Generating embeddings for AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf\n",
      "Generating embeddings for Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf\n",
      "Generating embeddings for Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "import os\n",
    "import tiktoken\n",
    "import hashlib\n",
    "\n",
    "docs_folder = \"../data/llms/\"\n",
    "formatted_chunks = []\n",
    "\n",
    "def hash_string(input_string):\n",
    "    input_bytes = input_string.encode(\"utf-8\")\n",
    "    return hashlib.sha256(input_bytes).hexdigest()\n",
    "\n",
    "for file in os.listdir(docs_folder):\n",
    "    print(f\"Generating embeddings for {file}\")\n",
    "\n",
    "    loader = PyPDFLoader(os.path.join(docs_folder, file))\n",
    "    pages = loader.load()\n",
    "\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    # from_tiktoken_encoder enables use to split on tokens rather than characters\n",
    "    recursive_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        encoding_name=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").name,\n",
    "        chunk_size=600, \n",
    "        chunk_overlap=125\n",
    "    )\n",
    "\n",
    "    recursive_text_splitter_chunks = recursive_text_splitter.split_documents(pages)\n",
    "    # Removing full path from filenames\n",
    "    for chunk in recursive_text_splitter_chunks:\n",
    "        chunk.metadata[\"file_name\"] = os.path.basename(chunk.metadata['source'])\n",
    "\n",
    "    chunk_content = [chunk.page_content for chunk in recursive_text_splitter_chunks]\n",
    "\n",
    "    recursive_text_splitter_embeddings = openai_client.embeddings.create(input=chunk_content, model=embedding_model_name)\n",
    "    recursive_text_splitter_embeddings = [result.embedding for result in recursive_text_splitter_embeddings.data]\n",
    "\n",
    "    \n",
    "    formatted_chunk = [\n",
    "        {\n",
    "            \"chunk_id\": f\"{hash_string(chunk.metadata['file_name'])}_{chunk.metadata['page']}_{i}\",\n",
    "            \"parent_id\": hash_string(chunk.metadata['file_name']),\n",
    "            \"chunk\": chunk.page_content,\n",
    "            \"title\": chunk.metadata[\"file_name\"],\n",
    "            \"vector\": recursive_text_splitter_embeddings[i]\n",
    "        }\n",
    "        for i, chunk in enumerate(recursive_text_splitter_chunks)\n",
    "    ]\n",
    "\n",
    "    formatted_chunks.append(formatted_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "Add texts and metadata from the JSON data to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunks for file Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf\n",
      "Uploaded chunks for file Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf\n",
      "Uploaded chunks for file LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "Uploaded chunks for file Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf\n",
      "Uploaded chunks for file Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf\n",
      "Uploaded chunks for file AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf\n",
      "Uploaded chunks for file Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf\n",
      "Uploaded chunks for file Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "for docs in formatted_chunks:    \n",
    "    result = search_client.upload_documents(docs)\n",
    "    print(f\"Uploaded chunks for file {docs[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload document using Azure AI search indexers (Alternative Approach)\n",
    "Documents are expected to be available in Azure ADLS storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerSkillset,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerDataContainer\n",
    ")\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    SplitSkill\n",
    ")\n",
    "\n",
    "def create_search_skillset(\n",
    "        skillset_name,\n",
    "        index_name,\n",
    "        azure_openai_endpoint,\n",
    "        azure_openai_embedding_deployment_id,\n",
    "        azure_openai_key=None,\n",
    "        text_split_mode='pages',\n",
    "        maximum_page_length=2000,\n",
    "        page_overlap_length=500):\n",
    "    return SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        skills=[\n",
    "            SplitSkill(\n",
    "                name=\"Text Splitter\",\n",
    "                default_language_code=\"en\",\n",
    "                text_split_mode=text_split_mode,\n",
    "                maximum_page_length=maximum_page_length,\n",
    "                page_overlap_length=page_overlap_length,\n",
    "                context=\"/document\",\n",
    "                inputs=[\n",
    "                    InputFieldMappingEntry(\n",
    "                        name=\"text\",\n",
    "                        source=\"/document/content\"\n",
    "                    )\n",
    "                ],\n",
    "                outputs=[\n",
    "                    OutputFieldMappingEntry(\n",
    "                        name=\"textItems\",\n",
    "                        target_name=\"pages\"\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            AzureOpenAIEmbeddingSkill(\n",
    "                name=\"Embeddings\",\n",
    "                resource_uri=azure_openai_endpoint,\n",
    "                deployment_id=azure_openai_embedding_deployment_id,\n",
    "                api_key=azure_openai_key, # Optional if using RBAC authentication\n",
    "                context=\"/document/pages/*\",\n",
    "                inputs=[\n",
    "                    InputFieldMappingEntry(\n",
    "                        name=\"text\",\n",
    "                        source=\"/document/pages/*\"\n",
    "                    )\n",
    "                ],\n",
    "                outputs=[\n",
    "                    OutputFieldMappingEntry(\n",
    "                        name=\"embedding\",\n",
    "                        target_name=\"vector\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        index_projections=SearchIndexerIndexProjections(\n",
    "            selectors=[\n",
    "                SearchIndexerIndexProjectionSelector(\n",
    "                    target_index_name=index_name,\n",
    "                    parent_key_field_name=\"parent_id\",\n",
    "                    source_context=\"/document/pages/*\",\n",
    "                    mappings=[\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"chunk\",\n",
    "                            source=\"/document/pages/*\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"vector\",\n",
    "                            source=\"/document/pages/*/vector\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"title\",\n",
    "                            source=\"/document/metadata_storage_name\"\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            parameters=SearchIndexerIndexProjectionsParameters(projection_mode=\"skipIndexingParentDocuments\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "search_indexer_client = SearchIndexerClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "        name=\"blob-source\",\n",
    "        type=\"azureblob\",\n",
    "        connection_string=blob_connection_string,\n",
    "        container=SearchIndexerDataContainer(\n",
    "            name=blob_container\n",
    "        )\n",
    "    )\n",
    "search_indexer_client.create_or_update_data_source_connection(data_source)\n",
    "\n",
    "skillset = create_search_skillset(\n",
    "    \"document-processor\",\n",
    "    index_name,\n",
    "    azure_openai_endpoint,\n",
    "    azure_openai_embedding_deployment,\n",
    "    azure_openai_key,\n",
    "    text_split_mode='pages',\n",
    "    maximum_page_length=2000,\n",
    "    page_overlap_length=500\n",
    ")\n",
    "\n",
    "search_indexer_client.create_or_update_skillset(skillset)\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "        name=\"document-indexer\",\n",
    "        data_source_name=data_source.name,\n",
    "        target_index_name=index_name,\n",
    "        skillset_name=skillset.name\n",
    "    )\n",
    "\n",
    "search_indexer_client.create_or_update_indexer(indexer)\n",
    "search_indexer_client.run_indexer(indexer.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "Score: 0.8611069\n",
      "Chunk: A P ROMPT ENGINEERING IN THE WILD\n",
      "Large models with natural language interfaces, including models for text generation and image\n",
      "synthesis, have seen an increasing amount of public usage in recent years. As ﬁnding the right prompt\n",
      "can be difﬁcult for humans, a number of guides on prompt engineering as well as tools to aid in\n",
      "prompt discovery have been developed. Among others, see, for example:\n",
      "•https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/\n",
      "•https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/\n",
      "•https://news.ycombinator.com/item?id=32943224\n",
      "•https://promptomania.com/stable-diffusion-prompt-builder/\n",
      "•https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion\n",
      "In this paper we apply APE to generate effective instructions for steering LLMs, but the general\n",
      "framework Algorithm 1 could be applied to steer other models with natural language interfaces so\n",
      "long as an appropriate proposal method and scoring function can be designed.\n",
      "15\n",
      "Title: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf\n",
      "Score: 0.84964734\n",
      "Chunk: 62B Model Output\n",
      "62B Model Output540B Model Output\n",
      "540B Model Output\n",
      "540B Model OutputQuestion\n",
      "Question\n",
      "QuestionFigure 10: Examples of semantic understanding and one-step missing errors that were ﬁxed by\n",
      "scaling PaLM from 62B to 540B.\n",
      "A.2 What is the role of prompt engineering?\n",
      "One of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage\n",
      "of work showing that prompts affect language models in unexpected ways (Min et al., 2022). The\n",
      "general way that we created chain of thought annotations was by taking eight exemplars from the\n",
      "training set and decomposing the reasoning process into multiple steps leading to the ﬁnal answer.\n",
      "Examples of chain of thought annotations are provided in Figure 3, with full prompts given in\n",
      "Appendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed\n",
      "robustness experiments with respect to various factors.\n",
      "•Different annotators. We ﬁrst analyze robustness to three different annotators (Section 3.4 and\n",
      "Figure 6). Although there is notable variance in performance (which we will discuss later), chain\n",
      "of thought performed better than the baseline by a large margin for all three annotators on eight\n",
      "datasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the\n",
      "annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instructions about\n",
      "17\n",
      "Title: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf\n",
      "Score: 0.8436833\n",
      "Chunk: C Extended Related Work\n",
      "Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\n",
      "ing, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\n",
      "intermediate language steps.\n",
      "C.1 Prompting\n",
      "The recent success of large-scale language models has led to growing interest in improving their\n",
      "capability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\n",
      "survey). This paper falls in the category of general prompting approaches, whereby input prompts are\n",
      "optimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\n",
      "2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\n",
      "One recent line of work aims to improve the ability of language models to perform a task by providing\n",
      "instructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\n",
      "et al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\n",
      "pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\n",
      "prepended to the inputs), chain-of-thought prompting augments the outputs of language models.\n",
      "Another related direction is sequentially combining the outputs of language models; human–computer\n",
      "interaction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\n",
      "language models improves task outcomes in a 20-person user study.\n",
      "C.2 Natural language explanations\n",
      "Another closely related direction uses natural language explanations (NLEs), often with the goal of\n",
      "improving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\n",
      "line of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\n",
      "2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\n",
      "prediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\n",
      "the chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\n",
      "NLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\n",
      "chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"prompt engineering approaches\"  \n",
    "  \n",
    "embedding = openai_client.embeddings.create(input=query, model=embedding_model_name).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    # filter=\"category eq 'Developer Tools'\",\n",
    "    select=[\"title\", \"chunk\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Chunk: {result['chunk']}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a pure vector search to demonstrate OpenAI's text-embedding-ada-002 multilingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "Score: 0.79798967\n",
      "Chunk: nett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Asso-\n",
      "ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n",
      "7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf .\n",
      "Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc\n",
      "Cary, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Bootstrapping inductive\n",
      "program synthesis with wake-sleep library learning. In Proceedings of the 42nd acm sigplan\n",
      "international conference on programming language design and implementation , pp. 835–850,\n",
      "2021.\n",
      "Tianyu Gao. Prompting: Better ways of using language models for nlp tasks. The Gradient , 2021.\n",
      "11\n",
      "Title: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf\n",
      "Score: 0.7968784\n",
      "Chunk: Quentin Lhoest, and Alexander M. Rush. 2020.\n",
      "Transformers: State-of-the-art natural language pro-\n",
      "cessing. In Proceedings of the 2020 Conference on\n",
      "Empirical Methods in Natural Language Processing:\n",
      "System Demonstrations , pages 38–45, Online. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Jeffrey O Zhang, Alexander Sax, Amir Zamir,\n",
      "Leonidas Guibas, and Jitendra Malik. 2020a. Side-\n",
      "tuning: A baseline for network adaptation via addi-\n",
      "tive side networks.\n",
      "Title: LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "Score: 0.7952175\n",
      "Chunk: A P ROMPT ENGINEERING IN THE WILD\n",
      "Large models with natural language interfaces, including models for text generation and image\n",
      "synthesis, have seen an increasing amount of public usage in recent years. As ﬁnding the right prompt\n",
      "can be difﬁcult for humans, a number of guides on prompt engineering as well as tools to aid in\n",
      "prompt discovery have been developed. Among others, see, for example:\n",
      "•https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/\n",
      "•https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/\n",
      "•https://news.ycombinator.com/item?id=32943224\n",
      "•https://promptomania.com/stable-diffusion-prompt-builder/\n",
      "•https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion\n",
      "In this paper we apply APE to generate effective instructions for steering LLMs, but the general\n",
      "framework Algorithm 1 could be applied to steer other models with natural language interfaces so\n",
      "long as an appropriate proposal method and scoring function can be designed.\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search multi-lingual (e.g 'tools for software development' in Dutch)  \n",
    "query = \"tools voor softwareontwikkeling\"  \n",
    "  \n",
    "embedding = openai_client.embeddings.create(input=query, model=embedding_model_name).data[0].embedding\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"vector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"chunk\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Chunk: {result['chunk']}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
